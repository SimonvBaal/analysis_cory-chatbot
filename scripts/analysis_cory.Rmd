---
title: "Cory - analysis"
author: "Simon van Baal"
date: "16/02/2022"
output: html_document
---

```{r setup, include=FALSE}

library(ordinal)
library(ordinalCont)
library(afex)
library(emmeans)
library(onewaytests)
library(rcompanion)

surveyData <- read_csv(here("./output/data/clean_surveys_cory.csv"))
chatbotData <- read_csv(here("./output/data/clean_chatbot_cory.csv"))
```

## Descriptive statistics

In this section, we start by providing some descriptive statistics about the sample, in addition to providing information about the distribution of the variables of interest.

```{r Descriptive statistics}

# Here we gather descriptive statistics by sex
surveyData %>%
  group_by(ParticipantID, Sex, Age) %>%
  summarise(n = n()) %>%
  group_by(Sex) %>%
  summarise(meanAge = mean(as.numeric(Age), na.rm = T),
            sdAge = sd(as.numeric(Age), na.rm = T),
            n = n())

# Some nationality data if needed
surveyData %>%
  group_by(Nationality) %>%
  summarise(n = n())

# Group level statistics on the perceived importance of testing
surveyData %>%
  group_by(Timepoint, Condition) %>%
  summarise(TestImportance = mean(TestImportance)) %>%
  arrange(Condition, desc(Timepoint))

# Group level statistics about the intention to get tested if the participant detects symptoms.
chatbotData %>%
  group_by(Condition, testingLikelihood) %>%
  summarise(n = n())
```

## Getting Tested when Experiencing COVID-19 Symptoms

Here we analyse whether the behavioural interventions and the interaction with the chatbot influenced people's likelihood of getting tested, and their perceived importance of getting tested.

```{r Analysis prep CLM}
# Ordering factor levels and scaling the age variable.
chatbotData <-
  chatbotData %>%
  mutate(
    testingLikelihood = factor(
      testingLikelihood,
      levels = c("Very Unlikely",
                 "I don't know",
                 "Very Likely")
    ),
    testingLikNum = ifelse(
      testingLikelihood ==
        "Very Unlikely",
      1,
      ifelse(
        testingLikelihood ==
          "I don't know",
        2,
        ifelse(testingLikelihood ==
                 "Very Likely", 3, NA)
      )
    ),
    Condition = factor(
      Condition,
      levels = c("Control",
                 "Compassion",
                 "Exponential Growth")
    ),
    Age = scale(Age, center = T, scale = T),
    Sex = factor(Sex)
  )


# Set contrasts for factor to sum contrasts
contrasts(chatbotData$Condition) <- contr.sum(3)
contrasts(chatbotData$Sex) <- contr.sum(2)

```


```{r Cumulative Link Model testing likelihood}

# Run cumulative link model

clmTestingLikelihood <- 
  clm(testingLikelihood ~ 
        Condition +
        Sex +
        Age,
       data = chatbotData %>% 
        filter(!is.na(testingLikelihood)),
      link = "logit")
# Hessian singular -> collapse categories (done above).

chiSqTestClm <- drop1(clmTestingLikelihood, test = "Chi")

# # Due to convergence issues, pairwise tests are not done through the model with emmeans
# emmeansClmTestingLikelihood <- 
#   emmeans(clmTestingLikelihood, specs = "Condition", mode = "mean.class")
# pairs(emmeansClmTestingLikelihood, reverse = T)


# Instead, we conduct simple Mann-Whitney U tests.
wTestExp <-
  wilcox.test(
    x = chatbotData$testingLikNum[chatbotData$Condition == 
                                    "Exponential Growth"],
    y = chatbotData$testingLikNum[chatbotData$Condition == 
                                    "Control"],
    paired = F,
    exact = F,
    conf.int = T,
    alternative = "greater"
  )

wTestComp <-
  wilcox.test(
    x = chatbotData$testingLikNum[chatbotData$Condition ==
                                    "Compassion"],
    y = chatbotData$testingLikNum[chatbotData$Condition ==
                                    "Control"],
    paired = F,
    exact = F,
    conf.int = T,
    alternative = "greater"
  )

ZValueWTestExp <-
  wilcoxonZ(
    x = chatbotData$testingLikNum[chatbotData$Condition == 
                                    "Exponential Growth"],
    y = chatbotData$testingLikNum[chatbotData$Condition == 
                                    "Control"],
    paired = F,
    exact = F,
    digits = 4
  )

zValueWTestComp <-
  wilcoxonZ(
    x = chatbotData$testingLikNum[chatbotData$Condition ==
                                    "Compassion"],
    y = chatbotData$testingLikNum[chatbotData$Condition ==
                                    "Control"],
    paired = F,
    exact = F,
    digits = 4
  )

adjustedPvalues <- p.adjust(c(.028, .165), method = "BH")



```

## Importance of Testing for COVID-19

Here we analyse whether the chatbot increases the perceived importance of testing as part of the public health response.

```{r Testing importance}

# Testing importance of getting tested before and after Cory.
testImportanceData <- 
  surveyData %>% 
  group_by(ParticipantID, Timepoint, Condition) %>%
  summarise(testImportance = mean(TestImportance)) %>%
  pivot_wider(id_cols = c("ParticipantID", "Condition"),
              names_from = "Timepoint",
              values_from = "testImportance") %>%
  mutate(testImportanceDifference = `Post-Test`-`Pre-Test`,
         Condition = factor(Condition, 
                            levels = c("Compassion", 
                                       "Exponential Growth", 
                                       "Control")))

contrasts(testImportanceData$Condition) <- contr.sum(3)


lmTestImportance <- 
  lm(
  testImportanceDifference ~
    Condition,
  data = testImportanceData
)




```

## Attitudes to Going Out

Here we analyse whether Cory influenced people's judgments of whether it is acceptable to leave the house or not in various scenarios.


```{r Certainty analysis prep}
# Add absolute value for certainty about going out.

surveyData <- 
  surveyData %>%
  mutate(AbsCertainty = abs(Attitude),
         scaledAge = scale(Age),
         Sex = factor(Sex),
         Timepoint = factor(Timepoint, 
                            levels = c("Pre-Test", 
                                       "Post-Test")))

contrasts(surveyData$Sex) <- contr.sum(2)

```

We conduct likelihood ratio tests to determine the best combination of the two variables of interest: Timepoint and Risk, where the two control variables Age and Sex remain in the model in each iteration.
The first test is whether the full model, with an interaction of risk level and timepoint, is the best model. Then we see whether each of these variables on their own constitute a significant improvement of the model.
```{r Certainty Likelihood Ratio Tests}
# Full model

ocmFullModelCertainty <-
  ocm(
    AbsCertainty ~
      Risk * Timepoint +
      Age +
      Sex +
      (1 | ParticipantID),
    surveyData %>%
      filter(!is.na(Age))
  )

# Reduced models

ocmReduced1Certainty <-
  ocm(
    AbsCertainty ~
      Risk + Timepoint +
      Age +
      Sex +
      (1 | ParticipantID),
    surveyData %>%
      filter(!is.na(Age))
  )

ocmReduced2Certainty <-
  ocm(AbsCertainty ~
        Risk +
        Age +
        Sex +
        (1 | ParticipantID),
      surveyData %>%
        filter(!is.na(Age)))

ocmReduced3Certainty <-
  ocm(AbsCertainty ~
        Timepoint +
        Age +
        Sex +
        (1 | ParticipantID),
      surveyData %>%
        filter(!is.na(Age)))

# Likelihood ratio tests

lrtCertainty <-
  anova(ocmFullModelCertainty,
        ocmReduced1Certainty,
        ocmReduced2Certainty,
        ocmReduced3Certainty)

# Now test for inclusion of the risk factor when time-point is in the model

lrtCertainty2 <-
  anova(ocmReduced1Certainty,
        ocmReduced3Certainty)

```


```{r Cont ordinal - attitudes to going out: pairwise}
# Now we estimate models with different reference levels in order to do pairwise
# comparisons.

# High Risk, Post-Test is the reference point here 
# (because of alphabetical ordering)
ocm1Certainty <- 
  ocm(AbsCertainty ~ 
          Risk*Timepoint + 
        Age +
        Sex +
          (1|ParticipantID),
         surveyData %>% 
        filter(!is.na(Age)))

# Here the difference between pre-test and post-test in high risk scenarios is:
#                               Estimate    StdErr t.value   p.value
#TimepointPost-Test             0.057005  0.130388 -0.4372 0.6632508 

ciLowerTimepointHigh = .057005 - .130388 * 1.96
ciUpperTimepointHigh = .057005 + .130388 * 1.96
surveyData %>%
  filter(!is.na(Age), Risk == "High") %>%
  group_by(Timepoint) %>%
  summarise(mean = mean(AbsCertainty),
            sd = sd(AbsCertainty))


#---------------------------------------- Low Risk

surveyData <-
  surveyData %>%
  mutate(Risk = factor(Risk, levels = c("Low", "High", "Minimal")))

# Now we run the model with low risk as the reference level.
ocm2Certainty <- 
  ocm(AbsCertainty ~ 
          Risk*Timepoint + 
        Age +
        Sex +
          (1|ParticipantID),
         surveyData %>% 
        filter(!is.na(Age)))

# Here the difference between pre-test and post-test in low risk scenarios is:
#                               Estimate    StdErr t.value   p.value
#TimepointPost-Test              -0.817518  0.183384  4.4579 2.902e-05 ***

ciLowerTimepointLow = -.817518 - .183384 * 1.96
ciUpperTimepointLow = -.817518 + .183384 * 1.96
surveyData %>%
  filter(!is.na(Age), Risk == "Low") %>%
  group_by(Timepoint) %>%
  summarise(mean = mean(AbsCertainty),
            sd = sd(AbsCertainty))


#--------------------------------------- Minimal Risk
surveyData <-
  surveyData %>%
  mutate(Risk = factor(Risk, levels = c("Minimal", "Low", "High")))

ocm3Certainty <- 
  ocm(AbsCertainty ~ 
          Risk*Timepoint + 
        Age +
        Sex +
          (1|ParticipantID),
         surveyData %>% 
        filter(!is.na(Age)))

# Here the difference between pre-test and post-test in minimal risk scenarios is:
#                            Estimate    StdErr t.value   p.value
#TimepointPost-Test           -1.238254  0.195481  6.3344 1.682e-08 ***

ciLowerTimepointMinimal = -1.238254 - .195481 * 1.96
ciUpperTimepointMinimal = -1.238254 + .195481 * 1.96
surveyData %>%
  filter(!is.na(Age), Risk == "Minimal") %>%
  group_by(Timepoint) %>%
  summarise(mean = mean(AbsCertainty),
            sd = sd(AbsCertainty))

```



```{r Certainty main effects analysis}
# For main effects we sequentially introduce sum contrasts.

# Set sum contrasts for risk, so we may evaluate the main effect of Timepoint.
contrasts(surveyData$Risk) <- contr.sum(3)

ocmMainTimepointCertainty <- 
  ocm(AbsCertainty ~ 
          Risk*Timepoint + 
        Age +
        Sex +
          (1|ParticipantID),
         surveyData %>% 
        filter(!is.na(Age)))
# The main effect of timepoint is:
#                          Estimate    StdErr t.value   p.value
#TimepointPost-Test       -0.666256  0.102592 -6.4943 8.578e-09 ***
ciLowerTimepointMain <- -0.666256 - 0.102592 * 1.96
ciUpperTimepointMain <- -0.666256 + 0.102592 * 1.96

surveyData %>%
  filter(!is.na(Age)) %>%
  group_by(Timepoint) %>%
  summarise(mean = mean(AbsCertainty),
            sd = sd(AbsCertainty))

# Set Risk back to treatment contrasts, and set Timepoint to sum contrasts,
# if you would like to evaluate the main effect of risk.
contrasts(surveyData$Risk) <- contr.treatment(3)
contrasts(surveyData$Timepoint) <- contr.sum(2)

ocmMainRiskCertainty <- 
  ocm(AbsCertainty ~ 
          Risk*Timepoint + 
        Age +
        Sex +
          (1|ParticipantID),
         surveyData %>% 
        filter(!is.na(Age)))
```

```{r Certainty effect size estimation}

# we extract coefficients from the model, turn them into odds ratios,
# and then we construct confidence intervals using standard errors.

estimateTimepointHigh <- exp(-ocm1Certainty$coefficients[4])

estimateTimepointLow <- exp(-ocm2Certainty$coefficients[4])

estimateTimepointMinimal <- exp(-ocm3Certainty$coefficients[4])


estimateMinimalHigh <- exp(-ocmTreatmentRisk$coefficients[5])
ciLowerMinimalHigh <- exp(-ocmTreatmentRisk$coefficients[5] - .0362*1.96)
ciUpperMinimalHigh <- exp(-ocmTreatmentRisk$coefficients[5] + .0362*1.96)


```


```{r Linear Model - attitudes to going out}

# Linear mixed model is used for creating a figure that reflects the model.
# This cannot be done easily with the ordinalCont package.

lmmCertainty <- 
  mixed(AbsCertainty ~ 
          Risk*Timepoint + 
          Age +
          Sex +
          (1|ParticipantID),
         surveyData)

emmeansCertainty <- 
  emmeans(lmmCertainty, ~ Timepoint|Risk)
pairs(emmeansCertainty, reverse = F)

```



```{r save image}
save.image()

```